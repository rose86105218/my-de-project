from datetime import datetime
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from airflow.operators.dummy_operator import DummyOperator
import random


# é è¨­åƒæ•¸
default_args = {
    'owner': 'data-team',
    'start_date': datetime(2024, 1, 1),
}


# ===== Python Functions =====

def step1_create_data(**context):
    """
    æ­¥é©Ÿ1 - å»ºç«‹è³‡æ–™
    ç¤ºç¯„ï¼šä½¿ç”¨ return è‡ªå‹•æ¨é€åˆ° XCom
    """
    print("ğŸ“ æ­¥é©Ÿ1 - å»ºç«‹è³‡æ–™")
    
    data = {
        'id': random.randint(1000, 9999),
        'name': random.choice(['å°æ˜', 'å°è¯', 'å°ç¾']),
        'value': random.randint(10, 100)
    }
    
    print(f"å»ºç«‹çš„è³‡æ–™ï¼š{data}")
    
    # return æœƒè‡ªå‹•å­˜åˆ° XCom
    return data


def step2_process_data(**context):
    """
    æ­¥é©Ÿ2 - è™•ç†è³‡æ–™
    ç¤ºç¯„ï¼šå¾ XCom æ‹‰å–è³‡æ–™ + æ‰‹å‹•æ¨é€
    """
    print("ğŸ”„ æ­¥é©Ÿ2 - è™•ç†è³‡æ–™")
    
    # å¾å‰ä¸€æ­¥å–å¾—è³‡æ–™
    data = context['task_instance'].xcom_pull(task_ids='step1_create_data')
    
    print(f"å–å¾—è³‡æ–™ï¼š{data}")
    
    # è™•ç†è³‡æ–™
    processed_value = data['value'] * 2
    result = {
        'original_value': data['value'],
        'processed_value': processed_value,
        'status': 'processed'
    }
    
    print(f"è™•ç†çµæœï¼š{result}")
    
    # æ‰‹å‹•æ¨é€åˆ° XCom
    context['task_instance'].xcom_push(key='processed_data', value=result)
    
    return "è³‡æ–™è™•ç†å®Œæˆ"


def step3_combine_data(**context):
    """
    æ­¥é©Ÿ3 - åˆä½µè³‡æ–™
    ç¤ºç¯„ï¼šæ‹‰å–å¤šå€‹ XCom è³‡æ–™
    """
    print("ğŸ”— æ­¥é©Ÿ3 - åˆä½µè³‡æ–™")
    
    ti = context['task_instance']
    
    # æ‹‰å–åŸå§‹è³‡æ–™
    original_data = ti.xcom_pull(task_ids='step1_create_data')
    # æ‹‰å–è™•ç†å¾Œçš„è³‡æ–™
    processed_data = ti.xcom_pull(task_ids='step2_process_data', key='processed_data')
    
    # åˆä½µè³‡æ–™
    combined_data = {
        'id': original_data['id'],
        'name': original_data['name'],
        'original_value': processed_data['original_value'],
        'final_value': processed_data['processed_value'],
        'summary': f"{original_data['name']} çš„è³‡æ–™å·²è™•ç†å®Œæˆ"
    }
    
    print(f"åˆä½µçµæœï¼š{combined_data}")
    
    return combined_data


# ===== DAG Definition =====

# ä½¿ç”¨ with DAG èªæ³•
with DAG(
    dag_id='example_xcom_coffee_shop_dag',
    default_args=default_args,
    description='å’–å•¡åº—è¨‚å–®ç³»çµ± - XCom è³‡æ–™å‚³éç¯„ä¾‹',
    schedule_interval=None,  # æ‰‹å‹•è§¸ç™¼
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['example', 'xcom', 'coffee'],
) as dag:

    # ===== Tasks Definition =====
    
    # é–‹å§‹ä»»å‹™
    start_task = DummyOperator(
        task_id='start',
    )
    
    # æ­¥é©Ÿ1ï¼šå»ºç«‹è³‡æ–™ - å±•ç¤º return è‡ªå‹•æ¨é€åˆ° XCom
    step1_task = PythonOperator(
        task_id='step1_create_data',
        python_callable=step1_create_data,
    )
    
    # æ­¥é©Ÿ2ï¼šè™•ç†è³‡æ–™ - å±•ç¤º XCom pull å’Œ push
    step2_task = PythonOperator(
        task_id='step2_process_data',
        python_callable=step2_process_data,
    )
    
    # æ­¥é©Ÿ3ï¼šåˆä½µè³‡æ–™ - å±•ç¤ºå¤šå€‹ XCom è³‡æ–™çš„ä½¿ç”¨
    step3_task = PythonOperator(
        task_id='step3_combine_data',
        python_callable=step3_combine_data,
    )
    
    # çµæŸä»»å‹™
    end_task = DummyOperator(
        task_id='end',
    )

    # ===== Task Dependencies =====
    # è¨­å®šä»»å‹™ä¾è³´é—œä¿‚ï¼šé–‹å§‹ â†’ æ­¥é©Ÿ1 â†’ æ­¥é©Ÿ2 â†’ æ­¥é©Ÿ3 â†’ çµæŸ
    start_task >> step1_task >> step2_task >> step3_task >> end_task
