[2025-09-10T22:37:09.428+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=97) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:37:09.430+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:37:09.434+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:09.433+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:37:09.462+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:37:10.100+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:10.099+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can delete on DAG:hahow_crawler_docker_producer_dag
[2025-09-10T22:37:10.120+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:10.120+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can read on DAG:hahow_crawler_docker_producer_dag
[2025-09-10T22:37:10.135+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:10.135+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can edit on DAG:hahow_crawler_docker_producer_dag
[2025-09-10T22:37:10.156+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:10.156+0800] INFO - override.py:1911 - create_permission() - Created Permission View: menu access on DAG Run:hahow_crawler_docker_producer_dag
[2025-09-10T22:37:10.172+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:10.171+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can create on DAG Run:hahow_crawler_docker_producer_dag
[2025-09-10T22:37:10.187+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:10.186+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can delete on DAG Run:hahow_crawler_docker_producer_dag
[2025-09-10T22:37:10.202+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:10.201+0800] INFO - override.py:1911 - create_permission() - Created Permission View: can read on DAG Run:hahow_crawler_docker_producer_dag
[2025-09-10T22:37:10.203+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:10.202+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:37:10.227+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:10.226+0800] INFO - dag.py:3262 - bulk_write_to_db() - Creating ORM DAG for hahow_crawler_docker_producer_dag
[2025-09-10T22:37:10.248+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:10.247+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:37:10.282+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.863 seconds
[2025-09-10T22:37:40.397+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=193) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:37:40.398+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:37:40.400+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:40.400+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:37:40.414+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:37:40.437+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:40.437+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:37:40.460+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:37:40.460+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:37:40.482+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.090 seconds
[2025-09-10T22:38:11.094+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=255) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:38:11.095+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:38:11.097+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:38:11.096+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:38:11.110+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:38:11.137+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:38:11.136+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:38:11.288+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:38:11.287+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:38:11.311+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.225 seconds
[2025-09-10T22:38:42.079+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=328) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:38:42.082+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:38:42.084+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:38:42.083+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:38:42.101+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:38:42.129+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:38:42.129+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:38:42.283+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:38:42.283+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:38:42.306+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.235 seconds
[2025-09-10T22:39:13.034+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=412) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:39:13.035+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:39:13.037+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:39:13.036+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:39:13.169+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:39:13.189+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:39:13.189+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:39:13.209+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:39:13.209+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:39:13.228+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.200 seconds
[2025-09-10T22:39:44.080+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=487) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:39:44.081+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:39:44.082+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:39:44.082+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:39:44.094+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:39:44.116+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:39:44.115+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:39:44.142+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:39:44.142+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:39:44.168+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.094 seconds
[2025-09-10T22:40:14.356+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=561) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:40:14.356+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:40:14.358+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:40:14.358+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:40:14.371+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:40:14.394+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:40:14.394+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:40:14.417+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:40:14.416+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:40:14.439+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.091 seconds
[2025-09-10T22:40:44.670+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=635) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:40:44.672+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:40:44.673+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:40:44.673+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:40:44.685+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:40:44.709+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:40:44.708+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:40:44.729+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:40:44.728+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:40:44.752+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.088 seconds
[2025-09-10T22:41:15.023+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=709) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:41:15.025+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:41:15.027+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:41:15.027+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:41:15.039+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:41:15.063+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:41:15.063+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:41:15.086+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:41:15.086+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:41:15.112+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.095 seconds
[2025-09-10T22:41:45.355+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=783) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:41:45.356+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:41:45.358+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:41:45.357+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:41:45.369+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:41:45.393+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:41:45.393+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:41:45.421+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:41:45.420+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:41:45.449+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.100 seconds
[2025-09-10T22:42:15.802+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=857) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:42:15.805+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:42:15.806+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:42:15.806+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:42:15.817+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:42:15.840+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:42:15.840+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:42:15.863+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:42:15.863+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:42:15.885+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.090 seconds
[2025-09-10T22:42:46.200+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=931) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:42:46.201+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:42:46.203+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:42:46.202+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:42:46.218+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:42:46.245+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:42:46.245+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:42:46.271+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:42:46.271+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:42:46.297+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.104 seconds
[2025-09-10T22:43:16.492+0800] INFO - processor.py:186 - _handle_dag_file_processing() - Started process (PID=1005) to work on /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:43:16.493+0800] INFO - processor.py:914 - process_file() - Processing file /opt/airflow/dags/hahow_crawler_docker_producer_dag.py for tasks to queue
[2025-09-10T22:43:16.494+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:43:16.494+0800] INFO - dagbag.py:588 - collect_dags() - Filling up the DagBag from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:43:16.508+0800] INFO - processor.py:925 - process_file() - DAG(s) 'hahow_crawler_docker_producer_dag' retrieved from /opt/airflow/dags/hahow_crawler_docker_producer_dag.py
[2025-09-10T22:43:16.534+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:43:16.534+0800] INFO - dag.py:3239 - bulk_write_to_db() - Sync 1 DAGs
[2025-09-10T22:43:16.558+0800] INFO - logging_mixin.py:190 - _propagate_log() - [2025-09-10T22:43:16.558+0800] INFO - dag.py:4180 - calculate_dagrun_date_fields() - Setting next_dagrun for hahow_crawler_docker_producer_dag to 2025-09-08 18:00:00+00:00, run_after=2025-09-09 18:00:00+00:00
[2025-09-10T22:43:16.582+0800] INFO - processor.py:208 - _run_file_processor() - Processing /opt/airflow/dags/hahow_crawler_docker_producer_dag.py took 0.099 seconds
